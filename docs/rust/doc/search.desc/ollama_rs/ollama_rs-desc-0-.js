searchState.loadedDescShard("ollama_rs", 0, "A trait to try to convert some type into a <code>Url</code>.\nCopy a model. Creates a model with another name from an …\nCreate a model with a single response, only the final …\nReturns a default Ollama instance with the host set to …\nDelete a model and its data.\nReturns the argument unchanged.\nCreate new instance from a <code>Url</code>.\nCompletion generation with a single response. Returns a …\nGenerate embeddings from a model\nCalls <code>U::from(self)</code>.\nCreates a new <code>Ollama</code> instance with the specified host and …\nCreates a new <code>Ollama</code> instance with the specified host, …\nPull a model with a single response, only the final status …\nUpload a model to a model library. Requires registering …\nChat message generation. Returns a <code>ChatMessageResponse</code> …\nChat message generation Returns a <code>ChatMessageResponse</code> …\nShow details about a model including modelfile, template, …\nAttempts to create a new <code>Ollama</code> instance from a URL.\nReturns the URI of the Ollama service as a <code>String</code>.\nReturns a reference to the URL of the Ollama service.\nReturns the URL of the Ollama service as a <code>&amp;str</code>.\nA coordinator for managing chat interactions and tool …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCreates a new <code>Coordinator</code> instance without tools.\nCreates a new <code>Coordinator</code> instance with tools.\nContains the error value\nRepresents an internal error within the Ollama service.\nContains the success value\nAn error type for the ollama-rs crate.\nA result type for operations in the ollama-rs crate.\nAn error type for tool call operations.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nModules related to generation tasks.\nThe creation time of the completion, in such format: …\nNumber of tokens the response\nTime in nanoseconds spent generating the response\nThe final data of the completion. This is only present if …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe generated chat message.\nThe name of the model used for the completion.\nNumber of tokens in the prompt\nTime spent in nanoseconds evaluating the prompt\nTime spent generating the response\nA chat message request to Ollama.\nThe format to return a response in.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAdditional model parameters listed in the documentation …\nThe full prompt or prompt template (overrides what is …\nTools that are available to the LLM.\nAn encoding of a conversation returned by Ollama after a …\nAn encoding of the conversation used in this response, …\nThe creation time of the completion, in such format: …\nWhether the completion is done. If the completion is …\nNumber of tokens in the response\nTime spent in nanoseconds generating the response\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe name of the model used for the completion.\nNumber of tokens in the prompt\nTime spent in nanoseconds evaluating the prompt\nThe response of the completion. This can be the entire …\nTime spent generating the response\nA generation request to Ollama.\nAdd an image to be used with the prompt\nThe context parameter returned from a previous request to …\nThe format to return a response in.\nReturns the argument unchanged.\nA list of images to be used with the prompt\nCalls <code>U::from(self)</code>.\nUsed to control how long a model stays loaded in memory, …\nCreates a new generation request with an suffix. Useful …\nAdditional model parameters listed in the documentation …\nAdds a text after the model response\nSystem prompt to (overrides what is defined in the …\nThe full prompt or prompt template (overrides what is …\nAn embeddings generation response from Ollama.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAn embeddings generation request to Ollama.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nOptions for generation requests to Ollama.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nEnable Mirostat sampling for controlling perplexity. …\nInfluences how quickly the algorithm responds to feedback …\nControls the balance between coherence and diversity of …\nSets the size of the context window used to generate the …\nThe number of layers to send to the GPU(s). On macOS it …\nThe number of GQA groups in the transformer layer. …\nMaximum number of tokens to predict when generating text. …\nSets the number of threads to use during computation. By …\nSets how far back for the model to look back to prevent …\nSets how strongly to penalize repetitions. A higher value …\nSets the random number seed to use for generation. Setting …\nSets the stop sequences to use. When this pattern is …\nThe temperature of the model. Increasing the temperature …\nTail free sampling is used to reduce the impact of less …\nReduces the probability of generating nonsense. A higher …\nWorks together with top-k. A higher value (e.g., 0.95) …\nThe format to return a response in\nA type which can be described as a JSON Schema document.\nRepresents a serialized JSON schema. You can create this …\nUsed to control how long a model stays loaded in memory, …\nRequires Ollama 0.5.0 or greater.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWhether JSON Schemas generated for this type should be …\nGenerates a JSON Schema for this type.\nGenerates a <code>RootSchema</code> for the given type using default …\nReturns a string that uniquely identifies the schema …\nThe name of the generated JSON Schema.\nIt’s highly recommended that the <code>JsonSchema</code> has …\nCall the tool. Note that returning an Err will cause it to …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA trait for managing chat message history.\nReturns a reference to the list of chat messages in the …\nAdds a chat message to the history.\nRepresents a local model pulled from Ollama.\nRepresents information about a model.\nModules related to model operations.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nA create model request to Ollama.\nA create model status response from Ollama.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a model described by the Modelfile contents passed …\nCreate a model described in the Modelfile at <code>path</code>.\nA pull model status response from Ollama.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nA push model status response from Ollama.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.")